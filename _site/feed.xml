<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The science of managing science</title>
    <description>Ideas about improving cooperation, process, and productivity in the creative sciences.
</description>
    <link>http://keenon.github.io/</link>
    <atom:link href="http://keenon.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 14 Aug 2015 14:29:00 -0700</pubDate>
    <lastBuildDate>Fri, 14 Aug 2015 14:29:00 -0700</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>The &#39;&#39;Cooperative Science&#39;&#39; Paradox</title>
        <description>&lt;p&gt;Imagine a tribe of hunter-gatherers living in the Great Plains of the American midwest, around year 1700. Buffallo are plentiful, and are the tribe’s primary source of food. A small number of tribe members manage the huts and everyday administration of the tribe, but the majority of the tribe’s members are hunters. Upon killing a buffallo, the individual responsible for throwing the spear that killed the buffallo is given tremendous glory, access to the best hut, and first pick on buffallo meat.&lt;/p&gt;

&lt;p&gt;In this tribe, hunters work alone, because the credits for buffallo-kill-assists are marginal at best. An assist, while helping the tribe overall, will yield the individual no hut, and hardly any glory. The chieftan of the tribe doesn’t know where fat buffallo may be hiding, and so she sends hunters off in every direction, to hedge against the failure of any one hunting expedition. If all the hunting expeditions fail for too many weeks in a row, stores will run low, and the tribe will starve.&lt;/p&gt;

&lt;p&gt;Substitue buffallo for papers, and we’ve got a reasonably accurate view of the academic working group in computer science.&lt;/p&gt;

&lt;p&gt;There’s a reason I chose such a primitive social structure to compare a lab to. I think we’re missing a fundamental cooperative “technology” to allow fruitful academic cooperation across individuals in order to tackle large scientific problems, and it’s within reach. In order to get there, though, we need to first understand where we are.&lt;/p&gt;

&lt;p&gt;Incentives in academia:&lt;/p&gt;

&lt;p&gt;It’s easy to see why the incentives for all the individuals in this scenario make it a stable point. A chieftan who moves all of the hunters into cooperating on a single hunt has trouble deciding how to allocate limited quality huts to a large successful group, and runs the risk of failure of all hunters during an expedition. It’s much safer to hedge and disperse. It also doesn’t make sense for hunters to unilaterally cooperate, except under very limited circumstances, because the gains from coopration are unclear and the rewards are given to a single individual.&lt;/p&gt;
</description>
        <pubDate>Fri, 14 Aug 2015 16:05:34 -0700</pubDate>
        <link>http://keenon.github.io/science/2015/08/14/teamwork.html</link>
        <guid isPermaLink="true">http://keenon.github.io/science/2015/08/14/teamwork.html</guid>
        
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>Better Testing for Bulletproof Software</title>
        <description>&lt;p&gt;TLDR: Quickcheck has a Java port that’s compatible with JUnit! It’s &lt;a href=&quot;https://github.com/pholser/junit-quickcheck&quot;&gt;on Github&lt;/a&gt;. I now use this for everything :)&lt;/p&gt;

&lt;p&gt;In research, we often report results from the outputs of fundamentally broken, overgrown, duct-tape programs written in some typeless language with a few half hearted unit tests, if any. By informal estimates from around the lab, we seem to believe we tend to lose between 2 and 5 percent output accuracy to completely preventable bugs, unrelated to the science we’re trying to prove about approaches. Those are huge numbers, and require radical changes to our testing habits to fix.&lt;/p&gt;

&lt;p&gt;Hackers (the breaking and entering kind, not the warm and fuzzy variety) are in the actual business of bug finding. Unlike the rest of us, who are in the business of bug creation, these professionals have tools that actually work for bug discovery. They have one technique in particular that I (until recently) had cause to envy, called ‘fuzzing’. Fuzzing works something like this: When faced with a new system that needs to be broken into, a black hat will write a script to pass random inputs to the system, and then she will run her script until the system crashes and she gets a stack trace. She then pokes around in the vicinity of the stack trace frames until she find a buffer whose bounds aren’t being checked, and then (I’m simplifying here) after a little copy paste from the famous &lt;a href=&quot;http://phrack.org/issues/49/14.html&quot;&gt;aleph1&lt;/a&gt;, it’s off to &lt;a href=&quot;https://www.torproject.org/projects/torbrowser.html.en&quot;&gt;Tor&lt;/a&gt; to find the highest bidder for her new &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-day_(computing)&quot;&gt;0 day&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It’s only recently that we bug creators have started to catch up with our own (more powerful) variant of fuzzing tools. Quickcheck (&lt;a href=&quot;http://www.cs.tufts.edu/~nr/cs257/archive/john-hughes/quick.pdf&quot;&gt;original paper&lt;/a&gt;) is a testing system that can be best explained as fuzz testing on crack, and for the good guys. The basic idea here is to write generators of random inputs, and a functional way to validate outputs are correct, and then let you test suite do the hard work of thinking up all the edge cases you forgot about. This is great, because you will get stack frames and inputs when the system finds a bug that crashes your system, but you’ll also get real confidence that your system produces outputs that conform to all sorts of functional invariants.&lt;/p&gt;

&lt;p&gt;The original Quickcheck was written for Haskell, but in practice real people don’t use functional languages much. There is a &lt;a href=&quot;https://github.com/pholser/junit-quickcheck&quot;&gt;JUnit Quickcheck port&lt;/a&gt; for the rest of us, and it’s what I recommend if you’re a Java person.&lt;/p&gt;

&lt;p&gt;Assuming you’re with me on the rough desirability of functional, randomly generated tests, then there’s one more problem that needs careful thought: don’t we have a chicken and egg problem here in terms of validating inputs? If we need to be able to tell if our program’s output is correct for any random input in order to write these tests, don’t the tests need as much functionality as the underlying program, thus needing tests themselves? What if your test and program have the same bug? Is it turtles all the way down?&lt;/p&gt;

&lt;p&gt;Here’s some guidelines I’ve developed to help me through these issues, and build confidence in my functional tests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use different algorithms for the test validity check and the program being tested, so the intersection of the possible bugs is a smaller space&lt;/li&gt;
  &lt;li&gt;Also write asserts for output invariants that don’t have to do with getting exactly the right answer, but that reduce the space of possible bugs even further&lt;/li&gt;
  &lt;li&gt;Use the slower (even intractible exhaustive enumeration) algorithm for the test, because those algorithms tend to be obviously correct by just reading. Then you can restrict that test to tractably small inputs, and be sure the answers are right for those inputs.&lt;/li&gt;
  &lt;li&gt;If you can generate the right answer along with the input somehow, always do that, rather than calculate in the test. This is rarely possible though.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’d like to hear everyone else’s thoughts about functional testing, and testing in general. How do you verify correctness? When do you write tests? How many tests do you need to feel confident? When do you decide to skip testing all together?&lt;/p&gt;

&lt;p&gt;Cheers,
Keenon&lt;/p&gt;
</description>
        <pubDate>Tue, 13 Jan 2015 03:53:34 -0800</pubDate>
        <link>http://keenon.github.io/opensource/2015/01/13/quickcheck.html</link>
        <guid isPermaLink="true">http://keenon.github.io/opensource/2015/01/13/quickcheck.html</guid>
        
        
        <category>opensource</category>
        
      </item>
    
      <item>
        <title>The &#39;&#39;Unsolved Problem&#39;&#39; Problem</title>
        <description>&lt;p&gt;Suppose your life depends on solving a problem that nobody in the world knows how to solve. Let’s make it concrete, and suppose that you’re about to get lung cancer from the pollution in Beijing, where you live and work as a bicycle messenger. You cannot afford a mask, yet you exercise and breathe heavily in the worst parts of down town Beijing for 12 hours a day, and the residue in your lungs is slowly building up to lethal levels. You cannot quit your job, because it is the best paid work available, and you family (who still live in the country side) are dependant on monthly payments you send home out of your paycheck. The problem you need to solve in a narrow sense is the too rapid approach of your death due to toxicity, and the effect your premature death would have on your family. In the grander sense, your problem is the smog in downtown Beijing, that has gotten so bad that you can’t see from one city block to the next.&lt;/p&gt;

&lt;p&gt;A natural (and socially modest) instinct might be to lose hope. After all, the world is a very large place, and if none of its 7.125 billion people have already solved this problem, it must be very hard indeed. Why should you believe you’re special in any way? Assuming a uniform prior on a given person seleced at random from humanity being able to solve this problem, then the empirical odds (if you’re a Bayesian statistician) put it at 7.125 billion (+1) to 1 that you (an average human with access to today’s technology) can solve this problem. Wouldn’t it be arrogant to assume you’d be the one to solve it?&lt;/p&gt;

&lt;p&gt;Perhaps it is, just a little, but it’s a socially responsible kind of arrogance. Plus there are things wrong with the psuedo-impossibility proof given in the previous paragraph. Yes, nobody has solved this particular problem yet, but judging the odds based on this problem alone is wildly misleading. People solve thousands of big problems a day, worldwide. The odds that a given person solves some problem, not necessarily this one, on given day are much higher than the perceived 10 billion to 1. The odds are still low, if the problem is a major one, but it is tractable and worth your time to try to solve it. Solving a major problem is closer to your odds of winning a chess tournament than winning the Powerball. The trick is to pick a problem, and give it a shot. And don’t worry, there are some tricks to solving problems, and they can be learned.&lt;/p&gt;

&lt;p&gt;So suppose that you can screw up your courage to believe for the briefest of moments that you’ll be able to accomplish something where everyone else has failed. What can you do? The first instinct is to start trying things as they occur to you: Think about your problem for a bit, come up with something that seems like it might work, try it, and if it fails, give up until you have another idea, and then start back at the beginning. In our example, this might be look like: tie a shirt around your head, and when it quickly becomes clear that this is doing very little to protect you, put the shirt back on, and wait until you think of something better. You might try stopping underneath a tree, on the theory that the tree is capturing smog and cleaning the air, but there aren’t enough trees on the routes you bike, and it doesn’t seem to help much, so you give up again.&lt;/p&gt;

&lt;p&gt;I’ve found that this “Undisciplined Method’’ (the haphazard trying of stuff) is surprisingly seductive for engineers trying to solve hard problems (among whom I count myself). It doesn’t require much in the way of planning or testing, which are easy to look at as overhead. It also lets us spend the vast majority of our time building things, which is what we love to do. It’s my suspicion that this tendency is a general human instinct, not just of interest to engineers.&lt;/p&gt;

&lt;p&gt;The fact that the Undisciplined Method is so seductive makes it that much harder to really internalize what I’m about to say: It hardly ever works. For a problem of even modest complexity (machine learning research, product development, fixing the smog in Beijing, etc.) your unguided intuition about what to try next is probably right &amp;lt;1% of the time, and you’ve got no meta-intuition about when that &amp;lt;1% is taking place.&lt;/p&gt;

&lt;p&gt;You might reasonably ask “Why is a low success-rate a problem? We’re working at the edge of human understanding here! We should be grateful for any progress we make at all.” You wouldn’t be wrong. This instinctive method was responsible for all human progress up through the Renaissance, and for much of progress since. But as with all things, there is a better way.&lt;/p&gt;

&lt;p&gt;Your most useful resource when you devote yourself to research is time. With infinite time, any reasonable strategy for knowledge discovery will work. But when there’s a paper deadline next weekend, or the smog is literally killing you, there’s a much more concrete need to optimize your research process. In order to do that, we need a little bit more meta-understanding.&lt;/p&gt;

&lt;p&gt;I learned about the “Scientific Method’’ in high school, and at the time dismissed it as so obvious as to be completely useless. My impression of it wasn’t helped by worksheets where I was asked to “Form a hypothesis about what will happen to the apple when we drop it.’’ I would respond “It will fall. This is the stupidest class ever.’’ In my experience, for one reason or another, many researchers and would-be entrepreneurs have similarly dismissed the “Scientific Method” in practice. They’ll still pay lip-service to its value, but they generally subscribe to the “Undisciplined Method” when pursuing solutions to hard problems.&lt;/p&gt;

&lt;p&gt;Contrary to my high school impressions, the Scientific Method is profound. The reason I felt it was useless in high school was because I was being asked to apply it to knowledge that was already well understood. How could a high school impression be otherwise? A lesson plan for 6th graders attempting to explain the scientific method can’t reasonably ask the 6th graders to probe the unsolved mysteries of the universe in any teachable, replicable way in just a 20 minute class period. When actually operating at the margins of human understanding, howevor, the Scientific Method forms a very strong foundation for success. This is best understood in contrast to the Undisciplined Method.&lt;/p&gt;

&lt;p&gt;The Undisciplined Method generates results poorly for several reasons: First, hypothesis and metrics to measure them are often poorly defined, so experimental validation is difficult and learning from failed experiments is hard. Second, there’s a lack of emphasis on data collection and understanding, for the dubious reason that it’s too time consuming to do so. This results in a poorer initial hypothesis on subsequent experiments. Third, there’s little or no thought at the beginning of an experiment to make sure that the information gained by the experiment is bought at a minimum cost of time and energy.&lt;/p&gt;

&lt;p&gt;The Scientific Method, when followed rigorously, addresses these issues. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Begin with understanding existing approaches, and collect data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Spend time understanding your data before forming your first hypothesis.&lt;/em&gt; This may feel like a waste of time initially. &lt;em&gt;It is not.&lt;/em&gt; A poor initial hypothesis wastes far more time than a few days of data analysis.&lt;/li&gt;
  &lt;li&gt;Form an educated, &lt;em&gt;falsifiable&lt;/em&gt; hypothesis about a solution to your problem. Think hard about the tests you will run, and the way you will diagnose and learn from this experiment if it fails to solve your problem.&lt;/li&gt;
  &lt;li&gt;Only then, build it, and run the experiment (this will still take the most time of any of these steps, absent a good framework to do this for you).&lt;/li&gt;
  &lt;li&gt;Run the error analysis you planned in advance. If the experiment proves a failure, which is the likely outcome, gather what information you can, incorporate that into your existing understanding of the problem, and go back to form another hypothesis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The hard part here is hypothesis formation, and there’s no way to do it right, so you’ll have to settle for doing it “pretty good’’ by making tons of measurements in advance of any hypothesis. Any idiot with sufficient data and time can come up with a reasonable idea of what to try. Testing that reasonable idea, as long as you have good measurements in place, will bring you that much closer to a solution, and if that still doesn’t work, try, try again.&lt;/p&gt;

&lt;p&gt;None of this is easy in practice. I’m consistently amazed by business people rediscovering the Scientific Method by another name, and thinking they’ve uncovered something new. “The Lean Startup’’, “Data-Driven Management’’, and “Design Thinking’’ are buzzwords that come to mind. Really they’re all just proposing “Step 1: Science, Step 2: Profit’’. The reason that they keep selling books is because, while the Scientific Method is tremendously powerful, it is counter-intuitive, and emotionally difficult. It’s much easier to lapse back into the Undisciplined Method than any active researcher cares to admit.&lt;/p&gt;

&lt;p&gt;So don’t. If you can muster the courage to try, and the discipline to follow through in the most efficient way, you could solve that hard problem, and it won’t take your whole career to do it. So, I’m waiting on you: Fix that massive smog issue, and then get us a Mars colony :)&lt;/p&gt;

&lt;p&gt;-Keenon&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Dec 2014 15:05:34 -0800</pubDate>
        <link>http://keenon.github.io/science/2014/12/10/unsolved-problem.html</link>
        <guid isPermaLink="true">http://keenon.github.io/science/2014/12/10/unsolved-problem.html</guid>
        
        
        <category>science</category>
        
      </item>
    
  </channel>
</rss>
