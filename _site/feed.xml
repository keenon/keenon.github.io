<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>There must be a better way</title>
    <description>Keenon Werling rants about things, and leaves it on the Internet
</description>
    <link>http://keenon.github.io/</link>
    <atom:link href="http://keenon.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 14 Dec 2014 19:12:07 -0800</pubDate>
    <lastBuildDate>Sun, 14 Dec 2014 19:12:07 -0800</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>The &#39;&#39;Unsolved Problem&#39;&#39; Problem</title>
        <description>&lt;p&gt;Suppose your life depends on solving a problem that nobody in the world knows how to solve. A natural (and modest) instinct might be to lose hope. After all, the world is a very large place, and if none of its 7.125 billion people have already solved this problem, it must be very hard indeed. Wouldn’t it be arrogant to assume you’d be the one to solve it?&lt;/p&gt;

&lt;p&gt;Perhaps it is, just a little, but it’s a socially responsible kind of arrogance. If nobody was ever self-confident enough to think that maybe they’d be the one to crack the puzzle, we’d still be sitting in caves, telling our kids that it is only God who can make fire. It requires a certain swaggering confidence to forego easy, low-risk, low-impact happiness (hunting boar) in favor of hard work, long hours, and a small chance of making a real contribution (setting out to make fire).&lt;/p&gt;

&lt;p&gt;So suppose that you can screw up your courage to believe for the briefest of moments that you’ll be able to accomplish something where everyone else has failed. What can you do? The first instinct is to start trying things as they occur to you: Think about your problem for a bit, come up with something that seems like it might work, try it, and if it fails, give up until you have another idea, and then start back at the beginning.&lt;/p&gt;

&lt;p&gt;I’ve found that this “Undisciplined Method’’ (the haphazard trying of stuff) is surprisingly seductive for engineers trying to solve hard problems (among whom I count myself). It doesn’t require much in the way of planning or testing, which are easy to look at as overhead. It also lets me spend the vast majority of my time building things, which is what I love to do. It’s my suspicion that this tendency is a general human instinct, not just of interest to engineers.&lt;/p&gt;

&lt;p&gt;The fact that the Undisciplined Method is so seductive makes it that much harder to really internalize what I’m about to say: It hardly ever works. For a problem of even modest complexity (machine learning research, product development, etc.) your intuition about what to try next is probably right 5% of the time, and you’ve got no meta-intuition about when that 5% is taking place.&lt;/p&gt;

&lt;p&gt;You might reasonably ask “Why is a low success-rate a problem? We’re working at the edge of human understanding here! You should be grateful for any progress you make at all.” You wouldn’t be wrong. This instinctive method was responsible for all human progress up through the Renaissance, and for much of progress since. But as with all things, there is a better way.&lt;/p&gt;

&lt;p&gt;Your most useful resource when you devote yourself to research is time. With infinite time, any reasonable strategy for knowledge discovery will work. But when there’s a paper deadline next weekend, there’s a much more concrete need to optimize your research process. In order to do that, we need a little bit more understanding.&lt;/p&gt;

&lt;p&gt;I learned about the “Scientific Method’’ in high school, and at the time dismissed it as so obvious as to be completely useless. My impression of it wasn’t helped by worksheets where I was asked to “Form a hypothesis about what will happen to the apple when we drop it.’’ I would respond “It will fall. This is the stupidest class ever.’’ In my experience, for one reason or another, many researchers and would-be entrepreneurs have similarly dismissed the Scientific Method.&lt;/p&gt;

&lt;p&gt;The Scientific Method is profound. The reason I felt it was useless in high school was because I was being asked to apply it to knowledge that was already well understood. When operating at the margins of human understanding, the Scientific Method forms a very strong foundation for success. This is best understood in contrast to the Undisciplined Method.&lt;/p&gt;

&lt;p&gt;The Undisciplined Method generates results poorly for several reasons: First, hypothesis and metrics to measure them are often poorly defined, so experimental validation is difficult and learning from failed experiments is hard. Second, there’s a lack of emphasis on data collection and understanding, for the dubious reason that it’s too time consuming to do so. This results in a poorer initial hypothesis on subsequent experiments. Third, there’s little or no thought at the beginning of an experiment to make sure that the information gained by the experiment is bought at a minimum cost of time and energy.&lt;/p&gt;

&lt;p&gt;The Scientific Method, when followed rigorously, addresses these issues. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Begin with understanding existing approaches, and collect data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Spend time understanding your data before forming your first hypothesis.&lt;/em&gt; This may feel like a waste of time initially. &lt;em&gt;It is not.&lt;/em&gt; A poor initial hypothesis wastes far more time than a few days of data analysis.&lt;/li&gt;
  &lt;li&gt;Form an educated, &lt;em&gt;falsifiable&lt;/em&gt; hypothesis about a solution to your problem. Think hard about the tests you will run, and the way you will diagnose and learn from this experiment if it fails to solve your problem.&lt;/li&gt;
  &lt;li&gt;Only then, build it, and run the experiment (this will still take the most time of any of these steps, absent a good framework to do this for you).&lt;/li&gt;
  &lt;li&gt;Run the error analysis you planned in advance. If the experiment proves a failure, which is the likely outcome, gather what information you can, incorporate that into your existing understanding of the problem, and go back to form another hypothesis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The hard part here is hypothesis formation, and there’s no way to do it right, so you’ll have to settle for doing it “pretty good’’ by making tons of measurements in advance of any hypothesis. Any idiot with sufficient data and time can come up with a reasonable idea of what to try. Testing that reasonable idea, as long as you have good measurements in place, will bring you that much closer to a solution, and if that still doesn’t work, try, try again.&lt;/p&gt;

&lt;p&gt;None of this is easy in practice. I’m consistently amazed by business people rediscovering the Scientific Method by another name, and thinking they’ve uncovered something new. “The Lean Startup’’, “Data-Driven Management’’, and “Design Thinking’’ are buzzwords that come to mind. Really they’re all just proposing “Step 1: Science, Step 2: Profit’’. The reason that they keep selling books is because, while the Scientific Method is tremendously powerful, it is counter-intuitive, and emotionally difficult. It’s much easier to lapse back into the Undisciplined Method than any active researcher cares to admit.&lt;/p&gt;

&lt;p&gt;So don’t. If you can muster the courage to try, and the discipline to follow through in the most efficient way, you could solve that hard problem, and it won’t take your whole career to do it :)&lt;/p&gt;

&lt;p&gt;Cheers,
Keenon&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Dec 2014 15:05:34 -0800</pubDate>
        <link>http://keenon.github.io/science/2014/12/10/unsolved-problem.html</link>
        <guid isPermaLink="true">http://keenon.github.io/science/2014/12/10/unsolved-problem.html</guid>
        
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>Real Valued Multi-Armed Bandits</title>
        <description>&lt;p&gt;Optimizing a modern digital product is complicated, labor intensive, and riddled with traps. The current state of the industry (though by no means state-of-the-art) is to use A/B testing to optimize websites. This involves getting a designer to make several different versions of something you’d like to optimize (say, a landing page), and then presenting each option to a randomly selected subset of the traffic to your site. By measuring visitors’ behavior on each of the versions, you can pick the one that maximizes profit for your business. The leader in the space has &lt;a href=&quot;https://www.optimizely.com/ab-testing/&quot;&gt;a great explanation of A/B testing&lt;/a&gt;, if you want more detail.&lt;/p&gt;

&lt;p&gt;The trouble is, A/B testing is hopelessly limited. You have to specify all your experiments in advance, and A/B testing can only be expected to pick the best among those options. It can’t point out a new option that you didn’t think of, which is better than anything you thought to try. In mathematical terms, my gripe with A/B testing is that it’s over &lt;em&gt;discrete space&lt;/em&gt;, and a very small discrete space at that.&lt;/p&gt;

&lt;p&gt;A better way wouldn’t require you to create options in advance. It would search over a whole enormous (potentially infinte) number of possible configurations, and pick the best one for you automatically. A pundit might call this “AI Creativity,” or “Machine Artwork,” but that would be an abuse of the terms “creativity” and “art.” Really, it’s all just automating trial and error, and a bit of tricky mathematics.&lt;/p&gt;

&lt;p&gt;On a small team, Amy Bearman, Bhaven Patel, and I (all Juniors at Stanford), cooked up a way to have a machine design the optimial digital business for you. The rest of this article is laying out what we did, hopefully in fairly understandable terms.&lt;/p&gt;

&lt;p&gt;Like all Machine Learning systems, this isn’t completely magic (just mostly). In order to use our system, you need to specify a set of things that the algorithm can change, in terms of numbers. For instance, you could let the system pick a font size for your landing page call to action. If you want the system to pick a color, you need to give the system 3 real values (red, green, and blue) to search over. In theory, you can give the system an unlimited number of parameters to search over, and with enough traffic it should work. In practice, the fewer parameters you give the system, the better a job it will do of jointly optimizing them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Proof In The Pudding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before we dive into math, graphs are always helpful. Below, there are 2 graphs. On both, I’ve used the same axis. The X and Y (the non-height axis) represent two features of your website you’re trying to optimize. For concreteness let’s say these axis represent the size of the call to action font, and darkness of the background, but in principle they could be any parameter that can be mapped by a number. The Z (height) at any combination of feature values (X and Y coordinates) represent the &lt;a href=&quot;http://en.wikipedia.org/wiki/Expected_value&quot;&gt;expected value&lt;/a&gt; at that combination. This is a measure from statistics representing the long run average amount of money we make per user, if we had a very large number of users visit the website.&lt;/p&gt;

&lt;p&gt;In order to test the math and theory that follows, we ran an experiment with a virtual population, because we didn’t have access to a website with a large flow of users (if you &lt;em&gt;have&lt;/em&gt; access to a large website, and feel like doing some science, feel free to get in touch). The following graph represents the preferences of our virtual population. We &lt;em&gt;made up this function&lt;/em&gt;, it has no bearing on reality. With a setting of “feature 1” at 2.0, and “feature 2” at 2.0, we have an expected value of $1.00 per user on average. With “feature 1” at 2.0, and “feature 2” at -2.0, we expect to lose an average of $1.00 per user (maybe this represents acquisition cost).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bandit/figure_2.png&quot; alt=&quot;Golden user preferences&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s a key point to grasp here. If we knew the true population preferences, we could optimize by finding the highest point on that graph. This is a tricky mathematical problem, but solvable (as we show below). The trouble is, our learning algorithm doesn’t know this in advance. In fact, our learning algorithm has no idea how users will respond to different settings of the parameters, a priori. We have to learn that through trial and error.&lt;/p&gt;

&lt;p&gt;Below is a graph of what the algorithm thinks about the population preferences, after only 30 virtual users visiting the site. Notice how amazingly close the algorithm’s guess about the virtual user preferences is to the true preferences. For now, ignore the lines on the plot. Those are charting how the computer can find the maximum value of the function, which we explain at length in the next section.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bandit/figure_3.png&quot; alt=&quot;Learned user preferences&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Gory Mathematical Details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We assume that there are a finite and known set of transitions that the user can take, each of which has a known and constant expected value. We assume that the user will behave stochastically. We also assume there exists some function that consistently maps the values of the parameters to the probability that the user will take a given action (click on a certain link), although the algorithm doesn’t know it. The analogy to the multi-armed bandit is loose, because the multi-armed bandit problem is about choosing from a number of discrete choices to maximize payout (unknown random payout distribution on each lever). In our casting of the problem, there are a number of known possible outcomes, each with known payout, but the function mapping some setting of real parameters to the probability of each outcome happing is unknown a priori. We draw the analogy anyways, because there is a conscious tradeoff between exploration and exploitation, and we borrow the simplest of the multi-armed bandit solutions: &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-greedy. This means with a probability &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; we choose a parameter setting at random, in order to learn about user behavior, and with probability &lt;script type=&quot;math/tex&quot;&gt;1-\epsilon&lt;/script&gt; we maximize our expected value.&lt;/p&gt;

&lt;p&gt;We approximate that function mapping parameter settings to user choice probabilities (and consequently expected value) with a set of SVMs, each responsible for mapping the probability that a single action is taken. We leave one SVM per choice. The probability of a choice &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; given a paremeter setting &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(i,x) = \frac{P_i(x)}{\sum{j = 1}^n P_j(x)}
&lt;/script&gt;

&lt;p&gt;If we pull apart the guts of the SVM, and remove the irritating normalization term, we have that &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
f(x) \propto \sum_{i = 1}^m a_iy_iK(x_i,x) + b
&lt;/script&gt;

&lt;p&gt;We have a set of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; children, where child &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has expected value &lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;. We are interested in our expected value, given a choice of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, representing the feature vector for creating a website.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
E(x) = \frac{\sum_{i=1}^nE_i(\sum_{j=1}^m a_j^{(i)}y_j^{(i)}K(x_j^{(i)},x) + b_i)}{\sum_{i=1}^n\sum_{j=1}^m a_j^{(i)}y_j^{(i)}K(x_j^{(i)},x) + b_i}
&lt;/script&gt;

&lt;p&gt;We wish to choose an &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; that maximizes our expected value under the model, given our evidence so far. This is non-convex, so we’d like a solution in closed form. Begin by taking the derivative&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx}E(x) = \frac{d}{dx}\frac{\sum_{i=1}^nE_i(\sum_{j=1}^m a_j^{(i)}y_j^{(i)}K(x_j^{(i)},x) + b_i)}{\sum_{i=1}^n\sum_{j=1}^m a_j^{(i)}y_j^{(i)}K(x_j^{(i)},x) + b_i}
&lt;/script&gt;

&lt;p&gt;In order to fit the entire derivative on one line, let:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
d(x) = \sum_{i=1}^n\sum_{j=1}^m a_j^{(i)}y_j^{(i)}K(x_j^{(i)},x)
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
e(x) = \sum_{i=1}^nE_i(\sum_{j=1}^m a_j^{(i)}y_j^{(i)}K(x_j^{(i)},x))
&lt;/script&gt;

&lt;p&gt;Then by the quotient rule, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx}E(x) = \frac{d(x)\frac{d}{dx}e(x) - e(x)\frac{d}{dx}d(x)}{(d(x))^2}
&lt;/script&gt;

&lt;p&gt;Following convention, let’s choose the Guassian kernel as our kernel:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
K(y,x) = e^{-\frac{||x-y||^2}{2\sigma^2}}
&lt;/script&gt;

&lt;p&gt;We’ll need the derivative also. We use the fact: &lt;script type=&quot;math/tex&quot;&gt;
||x-y||^2 = ||x||^2 - 2x^Ty + ||y||^2
&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx} K(y,x) = \frac{d}{dx} e^{-\frac{||x||^2 - 2x^Ty + ||y||^2}{2\sigma^2}}
&lt;/script&gt;

&lt;p&gt;Then we apply the chain rule and arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx} K(y,x) = (e^{-\frac{||x||^2 - 2x^Ty + ||y||^2}{2\sigma^2}})*(\frac{y - x}{\sigma^2})
&lt;/script&gt;

&lt;p&gt;And simplifying:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx} K(y,x) = e^{-\frac{||x-y||^2}{2\sigma^2}}(\frac{y - x}{\sigma^2})
&lt;/script&gt;

&lt;p&gt;Then we can substitute in and get our final derivative (written in pieces so it fits onto a sheet of paper):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
d(x) = \sum_{i=1}^n\sum_{j=1}^m a_j^{(i)}y_j^{(i)}e^{-\frac{||x-x_j^{(i)}||^2}{2\sigma^2}}
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx}d(x) = \sum_{i=1}^n\sum_{j=1}^m a_j^{(i)}y_j^{(i)}e^{-\frac{||x-x_j^{(i)}||^2}{2\sigma^2}}(\frac{x_j^{(i)} - x}{\sigma^2})
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
e(x) = \sum_{i=1}^nE_i(\sum_{j=1}^m a_j^{(i)}y_j^{(i)}e^{-\frac{||x-x_j^{(i)}||^2}{2\sigma^2}})
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx}e(x) = \sum_{i=1}^nE_i(\sum_{j=1}^m a_j^{(i)}y_j^{(i)}e^{-\frac{||x-x_j^{(i)}||^2}{2\sigma^2}}(\frac{x_j^{(i)} - x}{\sigma^2}))
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{d}{dx}E(x) = \frac{d(x)\frac{d}{dx}e(x) - e(x)\frac{d}{dx}d(x)}{(d(x))^2}
&lt;/script&gt;

&lt;p&gt;Our expectation in &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is non-concave and has many local optima. In practice, we use gradient descent with several restarts per problem to find the best value we can, with no optimality guarantees. Below is a graph of a gradient check on a toy dataset, with 6 potential user actions, each with a different expected value, resulting in 2 distinct (identical) peaks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bandit/figure_1.png&quot; alt=&quot;Gradient check for multi-armed bandit&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cheers,
Keenon&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Dec 2014 15:05:34 -0800</pubDate>
        <link>http://keenon.github.io/machine_learning/2014/12/10/real-valued-bandit.html</link>
        <guid isPermaLink="true">http://keenon.github.io/machine_learning/2014/12/10/real-valued-bandit.html</guid>
        
        
        <category>machine_learning</category>
        
      </item>
    
      <item>
        <title>On Reading History</title>
        <description>&lt;p&gt;I finished reading &lt;em&gt;The Invisible Bridge: The Fall of Nixon and Rise of Reagan&lt;/em&gt; by Rick Pearlstein a few weeks ago. It was an exceptionally partisan and generally untrustworthy account of recent history, but it contains an interesting throughline. To hear him tell it, once upon a time (in that great righteous WW2 blah blah blah), we were all patriotic, and the Presidency of the United States of America meant something. We won a wholly morally justified war against the Nazis, and with it an American Peace. We were the greatest nation on Earth, and we knew it. Then came Vietnam, and Watergate, and our dreams came tumbling down. Nixon was a monster and a criminal, and Americans didn’t believe in America anymore. The book goes on to argue that Reagan went about pulling wool back over American’s eyes, and that he postponed a still pending American “growing up”.&lt;/p&gt;

&lt;p&gt;Full disclosure: I vote Democratic. Even so, it’s hard to trust a book that leans so far left that it talks about Senate subcommitties being “larded by Republicans.” But the argument of the book aside, you often learn the most by examining the choice of “moral of the story” that history books are trying to validate, not their actual arguments, which come in the form of the main narrative. This book seems to be taking notice of a wave of cynicism in America (on this we agree). It would like us to assume with it that defeat in Vietnam was the inevitable fate for an irresonsible and immature superpower (probably also true). It would also like us to believe that a national “growing up” is possible only if Americans become more cynical (more dubious). I’m not completely sure about any of those claims, but aren’t they terribly interesting? Much more so than a discussion of Reagan’s early years as a lifeguard and radio announcer, which occupy a bulk of the pages.&lt;/p&gt;

&lt;p&gt;Cheers,
Keenon&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Dec 2014 15:05:34 -0800</pubDate>
        <link>http://keenon.github.io/culture/2014/12/10/reading-history.html</link>
        <guid isPermaLink="true">http://keenon.github.io/culture/2014/12/10/reading-history.html</guid>
        
        
        <category>culture</category>
        
      </item>
    
      <item>
        <title>America&#39;s Villain Complex</title>
        <description>&lt;p&gt;SPOILER ALERT: If you haven’t watched Season 1 of Game of Thrones, there’s a fairly substantial spoiler in here&lt;/p&gt;

&lt;p&gt;There are two TV shows I’d recommend you see: &lt;em&gt;Game of Thrones&lt;/em&gt;, and &lt;em&gt;House of Cards&lt;/em&gt;. If nothing else they say a lot about the American psyche.&lt;/p&gt;

&lt;p&gt;In both shows (which are both smash-hit successes), protaganists with hopelessly skewed moral complexes get what they want (which is always Power, with a capital P) by manipulating, lying, and cheating. We call this “realism,” and we love it. When a character in Game of Thrones flippantly says “Lord Stark was an honorable man, and Lord Stark died,” all of us watching at home nod in agreement. This is reality, we say. It’s nasty, people cheat to get what they want, and if you want to get ahead, you have to play dirty. If you’re too honorable and kind, you’ll never make it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.chzbgr.com/maxW500/7642998272/hEA77CF0F/&quot; alt=&quot;picture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pay special attention to that last sentence. If you didn’t make it, maybe it’s because you &lt;em&gt;were too honorable and kind.&lt;/em&gt; The hyper-cynical world-view we love to watch on TV has a strangely comforting upshot: those who lose in competition with others are endowed with moral virtue. It’s an ingenious way to reconcile the tautological fact that only 1% of the population can be in the top 1%, and Walt Disney telling you that you can be anything you want to be. Maybe that 1% are a bunch of sons-of-bitches, and the rest of us 99% are really just honorable souls who could’ve made it to the top, but would rather be kind to people.&lt;/p&gt;

&lt;p&gt;There may be some truth to it, and it certainly does comfort many, but I’m uncomfortable with the conclusions that really ambitious kids draw from this ethic that’s floating around.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You don’t have to be a Sadist to get ahead.&lt;/em&gt; In fact, people won’t work with a back-stabber, and if nobody will work with you, you’ll never take over Poland, no matter how hard you try. Don’t let me ruin your enjoyment, but take your TV with a grain of salt. It’s just a darker kind of Fairy Tale, and no less fanciful for being more brutal.&lt;/p&gt;

&lt;p&gt;Cheers,
Keenon&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Dec 2014 15:05:34 -0800</pubDate>
        <link>http://keenon.github.io/culture/2014/12/10/good-and-evil.html</link>
        <guid isPermaLink="true">http://keenon.github.io/culture/2014/12/10/good-and-evil.html</guid>
        
        
        <category>culture</category>
        
      </item>
    
  </channel>
</rss>
