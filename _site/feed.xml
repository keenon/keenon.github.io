<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Metascience</title>
    <description>Ideas about improving cooperation, process, and productivity in computer science research.
</description>
    <link>http://keenon.github.io/</link>
    <atom:link href="http://keenon.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 03 Sep 2015 16:57:36 -0500</pubDate>
    <lastBuildDate>Thu, 03 Sep 2015 16:57:36 -0500</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>Organizing Cooperative Science</title>
        <description>&lt;p&gt;I just got through &lt;a href=&quot;http://shop.oreilly.com/product/0636920025849.do&quot;&gt;Learning Agile&lt;/a&gt;, which is an excellent introduction to Agile and several of its manifestations: Scrum, Extreme Programming, and Kanban. It’s gotten me thinking about teamwork in the Stanford AI lab or, more accurately, its absence.&lt;/p&gt;

&lt;p&gt;In this post we’re going to go over how people build bombers, agile software development methods, and open source software development. Then we’ll discuss the need for a theory for collaboration in AI research&lt;/p&gt;

&lt;p&gt;As historical contrast for the modern project management science, let’s take a brief digression into the high stakes world of bomber design and manucturing. In &lt;a href=&quot;http://www.amazon.com/Skunk-Works-Personal-Memoir-Lockheed/dp/0316743003&quot;&gt;Skunk Works&lt;/a&gt; the author (who was the head of the famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Skunk_Works&quot;&gt;Lockheed Skunk Works&lt;/a&gt; during the developement of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Lockheed_F-117_Nighthawk&quot;&gt;Stealth Bomber&lt;/a&gt;) occassionally takes a break from war stories to talk about the design process they used to create their paradigm-breaking airplanes.&lt;/p&gt;

&lt;p&gt;First, a salesman/engineer talks to a few generals over at the Pentagon, and gets a sense of operational requirements for a new plane. These requirements are usually found in a strategic plan someplace, and have been carefully considered by teams of slide-rule-equipped analysts. They’re also dead simple: must have a range of X miles, with a cruising height of Y, and be able to carry a payload of Z.&lt;/p&gt;

&lt;p&gt;Then, a chief engineer sits down with the department heads (organized around critical engineering skills, like aerodynamics, thermodynamics, etc), and budgets out the rough specs for each component (landing gear, engines, wing lift, fuel tank capacity, etc). Appropriate parts are then found off-the-shelf whenever possible. Where not possible, the specs are given to more junior engineers, who design each component individually, with some guidance from the chief engineer. At the end, assuming everyone followed their specs, all the components are fit back together, shimmied a little to slot into place, and you’ve got a working plane (or atleast a design for one).&lt;/p&gt;

&lt;p&gt;Then, since only a few of each of these plane designs were ever produced (selling dozens would be considered a wild success), each one is assembled by hand. The Skunk Works had a (self-discribed) unique structure, at the time, where the engineers kept their desks in the same hangar as the manufacturing guys, and were in constant close communication as manufacturing realized that what had been drawn could not possibly be constructed as specified, and revisions were needed.&lt;/p&gt;

&lt;p&gt;Finally, the planes are built, and ready to be sailed off to the sunset (or Russia, as the case may be).&lt;/p&gt;

&lt;p&gt;Contrast that with the modern “Agile” approaches to software design, especially Scrum. Scrum is designed around the assumption that specifications are almost as difficult to write correctly as the software itself, and that the only way the customer will know that they don’t actually want what they asked for is for you to build it and show it to them.&lt;/p&gt;

&lt;p&gt;The method is to have the team meet for a full day at the beginning of every week to plan that week’s work. The work must culminate in something tangible that can be shown to customers. At the end of the week, the customer is presented with the work so far, and inevitably asks for a course correction.&lt;/p&gt;

&lt;p&gt;Then we can look at Open Source Software. OSS is a fascinating break from traditional management, because the engineers and contributors are also the users, and so any communication overhead between customers and engineers is unnecessary.&lt;/p&gt;

&lt;p&gt;That means that project evolution in the OSS space is driven organically as the result of cumulative pull requests. There may be an overall direction to the project, and a roadmap for future design set by stakeholders, but the daily reality is that the project is coordinate by a giant set of tickets, and code comes in as the result of (for the most part) individual engineers with ideas.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bombers and software are clearly very different beasts, both called “engineering,” yet these three processes are almost unrecognizably different. It’s worth contrasting the differences, to see if we can extrapolate to guidance for scientific cooperation.&lt;/p&gt;

&lt;p&gt;I’m going to invent some terminology here, though I’m almost certain the concepts these words refer to are not new (Google can’t turn up the papers I need yet though, so I’m not certain what’s come before). Let’s call the bomber design process “recursive enhancement,” and the agile software design process “iterative enhancement,” and open source software is “distributed enhancement.”&lt;/p&gt;

&lt;p&gt;Recursive enhancement:&lt;/p&gt;

&lt;p&gt;Bomber design involves a master engineer outlining rough specifications for a section of the machine and recursively asking junior engineers to enhance the specifications with details, constrained by specifications from above. This can unfurl to arbitrary levels of detail and organizational scales. This has the strength of accomodating large numbers of highly coordinated individuals. In order for this to work, general architectural patterns must be well understood at the outset.&lt;/p&gt;

&lt;p&gt;Iterative enhancement:&lt;/p&gt;

&lt;p&gt;Scrum software design allows frequent changes of direction, and requires collaboration in short term planning and execution that may not lend itself to clean recursive structure or large groups. The trouble is that the all-to-all communication of Scrum that is required at the beginning of every sprint limits scale of both the team and the problems tackled. That all-to-all communication distributes the load of software design across a lot of people, however, so allows for enormous amounts of planning and re-planning as things inevitably change when dealing with customers.&lt;/p&gt;

&lt;p&gt;Distributed enhancement:&lt;/p&gt;

&lt;p&gt;Starting from a basic shell produced by an intelligent design, distributed enhancement makes lots of little changes sum into project evolution. This is free to scale almost infinitely, but lacks the ability to centally coordinate onto a shared vision. A lot of energy is spent in making very small decisions, so projects move slowly, but the resulting software is usually extremely high quality. “With enough eyeballs, all bugs are shallow” - Linus Torvalds.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Going forward then, let’s look at the Stanford AI Lab. A few professors, each with a specific specialty, advise a few students each. The incentives are skewed toward publication above all else, and the cultural importance in our fields on the first author position leads to many breakdowns or false starts in cooperative effort.&lt;/p&gt;

&lt;p&gt;That being said, when cooperation does bear fruit, it does so in bulk. The Stanford &lt;a href=&quot;https://github.com/stanfordnlp/CoreNLP&quot;&gt;CoreNLP&lt;/a&gt; package is widely used as the standard preliminary toolset in Natural Language Processing, leading to dozens of citations for each of the components and lots of productivity for people within the lab working on top of a well understood, shared platform. Lately Bengio’s lab has been having a lot of success (their paper publication rate is enormous) with &lt;a href=&quot;https://github.com/mila-udem/blocks&quot;&gt;Blocks&lt;/a&gt;, their neural networks framework.&lt;/p&gt;

&lt;p&gt;I would estimate from personal experience that 95-98% of all work spent on AI research is spent on engineering large systems to implement ideas, and the other 2-5% is spent on math and managing runs. We often need to replicate the work of others, debug crappy open source to get valid comparisons, and generally suffer through enormous engineering slog. This work almost always is redundant with other systems, but the redundancies are never quite striking enough to justify the painful learning curve of using somebody else’s software.&lt;/p&gt;

&lt;p&gt;There are two reasons to figure out a way to collaborate anyways: within a group it will reduce redundant engineering (waste) and increase rate of publication, and between groups it will increase citations.&lt;/p&gt;

&lt;p&gt;In order to manage this collaboration effectively, we’ll need new methods to both overcome institutional inertia, and handle rapidly fluctuating interests and team structures within academia. That means we’ll have to learn from all the management methods that have come before, and work out the infrastructure to make collaboration efficient, sustainable, and useful.&lt;/p&gt;

&lt;p&gt;Here are a few things to ponder while I work on the sequel to this post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If we substitute Nature for customers, does Scrum actually amount to a team-based Scientific Method?&lt;/li&gt;
  &lt;li&gt;Can/should we standardize AI architectures to the point where the “recursize enhancement” strategies will work for new system construction?&lt;/li&gt;
  &lt;li&gt;Is there a productive way to divide AI into a sufficienty modular structure that code-sharing is useful?&lt;/li&gt;
  &lt;li&gt;How can we evolve team work to be more than sharing code? Can we reorganize credit systems to account for team assistance work?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To be continued…&lt;/p&gt;
</description>
        <pubDate>Fri, 14 Aug 2015 18:05:34 -0500</pubDate>
        <link>http://keenon.github.io/science/2015/08/14/teamwork.html</link>
        <guid isPermaLink="true">http://keenon.github.io/science/2015/08/14/teamwork.html</guid>
        
        
        <category>science</category>
        
      </item>
    
      <item>
        <title>Better Testing for Bulletproof Software</title>
        <description>&lt;p&gt;TLDR: Quickcheck has a Java port that’s compatible with JUnit! It’s &lt;a href=&quot;https://github.com/pholser/junit-quickcheck&quot;&gt;on Github&lt;/a&gt;. I now use this for everything :)&lt;/p&gt;

&lt;p&gt;In research, we often report results from the outputs of fundamentally broken, overgrown, duct-tape programs written in some typeless language with a few half hearted unit tests, if any. By informal estimates from around the lab, we seem to believe we tend to lose between 2 and 5 percent output accuracy to completely preventable bugs, unrelated to the science we’re trying to prove about approaches. Those are huge numbers, and require radical changes to our testing habits to fix.&lt;/p&gt;

&lt;p&gt;Hackers (the breaking and entering kind, not the warm and fuzzy variety) are in the actual business of bug finding. Unlike the rest of us, who are in the business of bug creation, these professionals have tools that actually work for bug discovery. They have one technique in particular that I (until recently) had cause to envy, called ‘fuzzing’. Fuzzing works something like this: When faced with a new system that needs to be broken into, a black hat will write a script to pass random inputs to the system, and then she will run her script until the system crashes and she gets a stack trace. She then pokes around in the vicinity of the stack trace frames until she find a buffer whose bounds aren’t being checked, and then (I’m simplifying here) after a little copy paste from the famous &lt;a href=&quot;http://phrack.org/issues/49/14.html&quot;&gt;aleph1&lt;/a&gt;, it’s off to &lt;a href=&quot;https://www.torproject.org/projects/torbrowser.html.en&quot;&gt;Tor&lt;/a&gt; to find the highest bidder for her new &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-day_(computing)&quot;&gt;0 day&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It’s only recently that we bug creators have started to catch up with our own (more powerful) variant of fuzzing tools. Quickcheck (&lt;a href=&quot;http://www.cs.tufts.edu/~nr/cs257/archive/john-hughes/quick.pdf&quot;&gt;original paper&lt;/a&gt;) is a testing system that can be best explained as fuzz testing on crack, and for the good guys. The basic idea here is to write generators of random inputs, and a functional way to validate outputs are correct, and then let you test suite do the hard work of thinking up all the edge cases you forgot about. This is great, because you will get stack frames and inputs when the system finds a bug that crashes your system, but you’ll also get real confidence that your system produces outputs that conform to all sorts of functional invariants.&lt;/p&gt;

&lt;p&gt;The original Quickcheck was written for Haskell, but in practice real people don’t use functional languages much. There is a &lt;a href=&quot;https://github.com/pholser/junit-quickcheck&quot;&gt;JUnit Quickcheck port&lt;/a&gt; for the rest of us, and it’s what I recommend if you’re a Java person.&lt;/p&gt;

&lt;p&gt;Assuming you’re with me on the rough desirability of functional, randomly generated tests, then there’s one more problem that needs careful thought: don’t we have a chicken and egg problem here in terms of validating inputs? If we need to be able to tell if our program’s output is correct for any random input in order to write these tests, don’t the tests need as much functionality as the underlying program, thus needing tests themselves? What if your test and program have the same bug? Is it turtles all the way down?&lt;/p&gt;

&lt;p&gt;Here’s some guidelines I’ve developed to help me through these issues, and build confidence in my functional tests:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use different algorithms for the test validity check and the program being tested, so the intersection of the possible bugs is a smaller space&lt;/li&gt;
  &lt;li&gt;Also write asserts for output invariants that don’t have to do with getting exactly the right answer, but that reduce the space of possible bugs even further&lt;/li&gt;
  &lt;li&gt;Use the slower (even intractible exhaustive enumeration) algorithm for the test, because those algorithms tend to be obviously correct by just reading. Then you can restrict that test to tractably small inputs, and be sure the answers are right for those inputs.&lt;/li&gt;
  &lt;li&gt;If you can generate the right answer along with the input somehow, always do that, rather than calculate in the test. This is rarely possible though.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’d like to hear everyone else’s thoughts about functional testing, and testing in general. How do you verify correctness? When do you write tests? How many tests do you need to feel confident? When do you decide to skip testing all together?&lt;/p&gt;

&lt;p&gt;Cheers,
Keenon&lt;/p&gt;
</description>
        <pubDate>Tue, 13 Jan 2015 05:53:34 -0600</pubDate>
        <link>http://keenon.github.io/opensource/2015/01/13/quickcheck.html</link>
        <guid isPermaLink="true">http://keenon.github.io/opensource/2015/01/13/quickcheck.html</guid>
        
        
        <category>opensource</category>
        
      </item>
    
      <item>
        <title>The &#39;&#39;Unsolved Problem&#39;&#39; Problem</title>
        <description>&lt;p&gt;Suppose your life depends on solving a problem that nobody in the world knows how to solve. Let’s make it concrete, and suppose that you’re about to get lung cancer from the pollution in Beijing, where you live and work as a bicycle messenger. You cannot afford a mask, yet you exercise and breathe heavily in the worst parts of down town Beijing for 12 hours a day, and the residue in your lungs is slowly building up to lethal levels. You cannot quit your job, because it is the best paid work available, and you family (who still live in the country side) are dependant on monthly payments you send home out of your paycheck. The problem you need to solve in a narrow sense is the too rapid approach of your death due to toxicity, and the effect your premature death would have on your family. In the grander sense, your problem is the smog in downtown Beijing, that has gotten so bad that you can’t see from one city block to the next.&lt;/p&gt;

&lt;p&gt;A natural (and socially modest) instinct might be to lose hope. After all, the world is a very large place, and if none of its 7.125 billion people have already solved this problem, it must be very hard indeed. Why should you believe you’re special in any way? Assuming a uniform prior on a given person seleced at random from humanity being able to solve this problem, then the empirical odds (if you’re a Bayesian statistician) put it at 7.125 billion (+1) to 1 that you (an average human with access to today’s technology) can solve this problem. Wouldn’t it be arrogant to assume you’d be the one to solve it?&lt;/p&gt;

&lt;p&gt;Perhaps it is, just a little, but it’s a socially responsible kind of arrogance. Plus there are things wrong with the psuedo-impossibility proof given in the previous paragraph. Yes, nobody has solved this particular problem yet, but judging the odds based on this problem alone is wildly misleading. People solve thousands of big problems a day, worldwide. The odds that a given person solves some problem, not necessarily this one, on given day are much higher than the perceived 10 billion to 1. The odds are still low, if the problem is a major one, but it is tractable and worth your time to try to solve it. Solving a major problem is closer to your odds of winning a chess tournament than winning the Powerball. The trick is to pick a problem, and give it a shot. And don’t worry, there are some tricks to solving problems, and they can be learned.&lt;/p&gt;

&lt;p&gt;So suppose that you can screw up your courage to believe for the briefest of moments that you’ll be able to accomplish something where everyone else has failed. What can you do? The first instinct is to start trying things as they occur to you: Think about your problem for a bit, come up with something that seems like it might work, try it, and if it fails, give up until you have another idea, and then start back at the beginning. In our example, this might be look like: tie a shirt around your head, and when it quickly becomes clear that this is doing very little to protect you, put the shirt back on, and wait until you think of something better. You might try stopping underneath a tree, on the theory that the tree is capturing smog and cleaning the air, but there aren’t enough trees on the routes you bike, and it doesn’t seem to help much, so you give up again.&lt;/p&gt;

&lt;p&gt;I’ve found that this “Undisciplined Method’’ (the haphazard trying of stuff) is surprisingly seductive for engineers trying to solve hard problems (among whom I count myself). It doesn’t require much in the way of planning or testing, which are easy to look at as overhead. It also lets us spend the vast majority of our time building things, which is what we love to do. It’s my suspicion that this tendency is a general human instinct, not just of interest to engineers.&lt;/p&gt;

&lt;p&gt;The fact that the Undisciplined Method is so seductive makes it that much harder to really internalize what I’m about to say: It hardly ever works. For a problem of even modest complexity (machine learning research, product development, fixing the smog in Beijing, etc.) your unguided intuition about what to try next is probably right &amp;lt;1% of the time, and you’ve got no meta-intuition about when that &amp;lt;1% is taking place.&lt;/p&gt;

&lt;p&gt;You might reasonably ask “Why is a low success-rate a problem? We’re working at the edge of human understanding here! We should be grateful for any progress we make at all.” You wouldn’t be wrong. This instinctive method was responsible for all human progress up through the Renaissance, and for much of progress since. But as with all things, there is a better way.&lt;/p&gt;

&lt;p&gt;Your most useful resource when you devote yourself to research is time. With infinite time, any reasonable strategy for knowledge discovery will work. But when there’s a paper deadline next weekend, or the smog is literally killing you, there’s a much more concrete need to optimize your research process. In order to do that, we need a little bit more meta-understanding.&lt;/p&gt;

&lt;p&gt;I learned about the “Scientific Method’’ in high school, and at the time dismissed it as so obvious as to be completely useless. My impression of it wasn’t helped by worksheets where I was asked to “Form a hypothesis about what will happen to the apple when we drop it.’’ I would respond “It will fall. This is the stupidest class ever.’’ In my experience, for one reason or another, many researchers and would-be entrepreneurs have similarly dismissed the “Scientific Method” in practice. They’ll still pay lip-service to its value, but they generally subscribe to the “Undisciplined Method” when pursuing solutions to hard problems.&lt;/p&gt;

&lt;p&gt;Contrary to my high school impressions, the Scientific Method is profound. The reason I felt it was useless in high school was because I was being asked to apply it to knowledge that was already well understood. How could a high school impression be otherwise? A lesson plan for 6th graders attempting to explain the scientific method can’t reasonably ask the 6th graders to probe the unsolved mysteries of the universe in any teachable, replicable way in just a 20 minute class period. When actually operating at the margins of human understanding, howevor, the Scientific Method forms a very strong foundation for success. This is best understood in contrast to the Undisciplined Method.&lt;/p&gt;

&lt;p&gt;The Undisciplined Method generates results poorly for several reasons: First, hypothesis and metrics to measure them are often poorly defined, so experimental validation is difficult and learning from failed experiments is hard. Second, there’s a lack of emphasis on data collection and understanding, for the dubious reason that it’s too time consuming to do so. This results in a poorer initial hypothesis on subsequent experiments. Third, there’s little or no thought at the beginning of an experiment to make sure that the information gained by the experiment is bought at a minimum cost of time and energy.&lt;/p&gt;

&lt;p&gt;The Scientific Method, when followed rigorously, addresses these issues. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Begin with understanding existing approaches, and collect data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Spend time understanding your data before forming your first hypothesis.&lt;/em&gt; This may feel like a waste of time initially. &lt;em&gt;It is not.&lt;/em&gt; A poor initial hypothesis wastes far more time than a few days of data analysis.&lt;/li&gt;
  &lt;li&gt;Form an educated, &lt;em&gt;falsifiable&lt;/em&gt; hypothesis about a solution to your problem. Think hard about the tests you will run, and the way you will diagnose and learn from this experiment if it fails to solve your problem.&lt;/li&gt;
  &lt;li&gt;Only then, build it, and run the experiment (this will still take the most time of any of these steps, absent a good framework to do this for you).&lt;/li&gt;
  &lt;li&gt;Run the error analysis you planned in advance. If the experiment proves a failure, which is the likely outcome, gather what information you can, incorporate that into your existing understanding of the problem, and go back to form another hypothesis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The hard part here is hypothesis formation, and there’s no way to do it right, so you’ll have to settle for doing it “pretty good’’ by making tons of measurements in advance of any hypothesis. Any idiot with sufficient data and time can come up with a reasonable idea of what to try. Testing that reasonable idea, as long as you have good measurements in place, will bring you that much closer to a solution, and if that still doesn’t work, try, try again.&lt;/p&gt;

&lt;p&gt;None of this is easy in practice. I’m consistently amazed by business people rediscovering the Scientific Method by another name, and thinking they’ve uncovered something new. “The Lean Startup’’, “Data-Driven Management’’, and “Design Thinking’’ are buzzwords that come to mind. Really they’re all just proposing “Step 1: Science, Step 2: Profit’’. The reason that they keep selling books is because, while the Scientific Method is tremendously powerful, it is counter-intuitive, and emotionally difficult. It’s much easier to lapse back into the Undisciplined Method than any active researcher cares to admit.&lt;/p&gt;

&lt;p&gt;So don’t. If you can muster the courage to try, and the discipline to follow through in the most efficient way, you could solve that hard problem, and it won’t take your whole career to do it. So, I’m waiting on you: Fix that massive smog issue, and then get us a Mars colony :)&lt;/p&gt;

&lt;p&gt;-Keenon&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Dec 2014 17:05:34 -0600</pubDate>
        <link>http://keenon.github.io/science/2014/12/10/unsolved-problem.html</link>
        <guid isPermaLink="true">http://keenon.github.io/science/2014/12/10/unsolved-problem.html</guid>
        
        
        <category>science</category>
        
      </item>
    
  </channel>
</rss>
